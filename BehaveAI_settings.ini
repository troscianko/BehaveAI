[DEFAULT]
# Motion/behaviour classes (separated by commas, colours seprated by semicolon):
primary_motion_classes = sym, assym, weak
primary_motion_colors = 143,193,193; 143,240,164; 246,97,81
primary_motion_hotkeys = s, a, w
motion_blocks_static = false

secondary_motion_classes = 0
secondary_motion_colors = 0
secondary_motion_hotkeys = 0

primary_static_classes = 0
primary_static_colors = 0
primary_static_hotkeys = 0
static_blocks_motion = false

secondary_static_classes = 0
secondary_static_colors = 0
secondary_static_hotkeys = 0

ignore_secondary =
save_empty_frames = true
dominant_source = motion

# some nice colours to use above:
# 220,138,221; 249,240,107; 246,97,81; 143,193,193; 153,193,241; 143,240,164

# -------------------------PRIMARY VS SECONDARY-------------------------
# Primary classifiers use the entire frame to search for classes. You must always have one primary classifier,
# whether motion (false-colour motion images) or static (unmodified still frames with notmal RGB colour).
# For stationary object deteciton use a primary static (as many classses as you need). For detecting moving
# objects, use a primary motion classifier. It's probably best to use as few classifiers at this stage as possible
# it's more processor intensive, and try to make sure the classes are all sufficiently different that they can
# easily be differentiated.
# Secondary classifiers are applied to cropped images of anything detected by a primary classifier. e.g. a male or female
# fly is detected with a primary classifier, then the fly is cropped and fed into a secondary motion classifier that
# can integrate motion cues - e.g. determining whether it is dispalying, walking, resting etc... The options
# you select here are automatically loaded into the annotaiton and classification scripts. The annotation script
# automatically handles the database creation. The classification script manages the model training and batch
# annotates any videos in an 'input' direcotry in the working directory. It creates an output video showing
# annotations, and creates a .csv file that details the output.

# IMPORTANT - the annotation, training, and detection/classification stages must all use identical settings for
# motion false colour creation (expA, expB, scale_factor, strategy, rgb_multipliers) and classifier information
# (primary and secondary classes). Other settings (Kalman filter, iou, threaholds etc... can all be altered in order
# to optimise the tracking performance.

# -------------------------HOTKEY TIPS-------------------------
# Use '0' (zero) for any unused classifiers
# Hotkeys must avoid 'u' (undo)
# Where there is a primary classifier that matches a secondary classifier, give them the same hotkey
# and the code then avoids making a secondary classifier for that case. e.g. insects in flight can't
# be sexed to a male or female primary static class, so 'fly' can be set as both a primary motion class
# and a secondary motion class, giving both the same hotkey 'f'. Now 'fly' will be treated as a non-
# sexed primary and secondary, and during annotation this becomes implict.

# -------------------------BLOCKING-------------------------
# If cross-blocking is enabled it greys out the primary motion from static training images and vice-versa.
# This prevents the models from training in each-others presence, which could be useful in some circumstances
# to increase sensitivity (but may also result in cross-talk between models - each seeing the same thing repeatedly)
# E.g. if motion_blocks_static = true and static_blocks_motion = false, any primary motion classes selected won't be
# visible to the static model when training. But primary static classes will still be visible to the motion model
# when training. If the logic here is confusing, then run a few sample slides and check the annotation output
# images and labels to make sure it's doing what you want.


# Dominant source: ('confidence', 'static', or 'motion') specifies which source should be given priority in the video
# labels - note that the CSV output will save all sources simultaneously.


# All images can be down-scaled. Downscaling can make the classifier much faster, but ideally just use 1.0 (no scaling)
scale_factor = 1.0

# using frame skipping allows the script to ignore the set number of frames, effectively allowing the systpe to look further
# back in time and analyse fewer frames. Set to zero to turn off, 1 would only process every other frame, 2 would only 
# process every 3rd frame
frame_skip = 0

# The motion threshold value is subtracted from the motion image. Only motion above this threshold will therefore be
# seen in the motion output. But too high and you'll lose important motion information.
# The effect of this is most visible if you've set lum_weight to zero (0.0). The range is 0 to 255.
motion_threshold = 0

# rendering line thickness (increase for high-res video) line-thickness must be integer, font_size can be decimal
# e.g. for HD video use 1 & 0.5, for 4k video use 2 & 1
line_thickness = 1
font_size = 0.5

# Validation frequency - annotation frames are randomly allocated to validation
# 0.1 would send ~10% for validation on average
val_frequency = 0.2

# An 'exponential' strategy exponentially smooths the difference frames (specified by expA and expB)
# and also the previous frames; this should be better for nuanced motion
# A 'sequeential' strategy only shows the difference frames without smoothing
# should be better at detecting/classifying very fast moving things
strategy = exponential

# expA and expB are the exponential smoothing parameters. A value of 0.8 means the previous
# difference frame will have a weighting of 0.8, so will preserve older information
# than a lower value. Using 'discrete' strategy doesn't use these values
expA = 0.5
expB = 0.8

# higher lum_weight values preserve more of the frame's luminance information, so the model can see more static detail. Lower values weight towards motion-only information
lum_weight = 0.0

# adjust these values to balance the intensity of the red, green and blue components
rgb_multipliers = 6, 6, 6

# Select classifier - e.g. primary should be yolov8s.pt, yolo11s.pt, yolo11n.pt etc...
# secondary should be yolo11s-cls.pt, yolov8s-cls.pt etc... i.e. version 8 or 11, and size (n=nano, s=small, m=medium etc...)
# note that yolov8 has the 'v' but yolo 11 doesn't. 
primary_classifier = yolo11s.pt
primary_epochs = 50
secondary_classifier = yolo11s-cls.pt
secondary_epochs = 50


# threshold confidence for accepting classification:
primary_conf_thresh = 0.5
secondary_conf_thresh = 0.5

# maximum distance targets can move between frames relative to the esimated location (based on
# previous direction and speed)
match_distance_thresh = 200

# how many frames after detection shoudl a classified object's location be forgotten? 
delete_after_missed = 5

# If two things are detected in the same position, how close shoudl they be in ordet ot be combined?
centroid_merge_thresh = 50

# if the intersection of two boxes is above this threshold, they are combined 
iou_thresh = 0.4


[kalman]
#increase if objects move erratically - default 0.01
#~ process_noise_pos = 0.1
process_noise_pos = 1

#increase if objects change speed/direction frequently - default 0.1
#~ process_noise_vel = 0.5
process_noise_vel = 0.05

#increase if detections are noisy/jumpy - default 0.1
#~ measurement_noise = 1.0
measurement_noise = 0.5


